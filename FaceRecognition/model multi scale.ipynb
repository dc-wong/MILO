{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python tqdm numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDDB Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 171/102060 [00:05<51:45, 32.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 190\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x_min, y_min, x_max, y_max) \u001b[38;5;129;01min\u001b[39;00m padded_bboxes:\n\u001b[0;32m    188\u001b[0m     label_out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 459\u001b[0m     fp_write(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m len_s, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 453\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py:578\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_timeout):\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# CONFIG\n",
    "# ===============================================\n",
    "\n",
    "# Paths\n",
    "dataset_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\images\"\n",
    "labels_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\label.txt\"\n",
    "output_dir  = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\"  # Where to save results\n",
    "\n",
    "output_label_file = os.path.join(output_dir, \"all_augmentations.txt\")\n",
    "\n",
    "# Target resolution\n",
    "TARGET_HEIGHT, TARGET_WIDTH = 1080, 1920\n",
    "\n",
    "# Define a list of scales (feel free to adjust)\n",
    "SCALES = [0.5, 1.0]\n",
    "SCALES.extend([1/scale for scale in SCALES])\n",
    "\n",
    "# Define alignment anchors for both axes\n",
    "# e.g. 0 → align to the \"top\" or \"left\", 0.5 → align \"center\", 1.0 → align \"bottom\" or \"right\".\n",
    "# You could also do 0, 0.25, 0.5, 0.75, 1.0 to get 5 steps for each axis.\n",
    "ALIGNMENT_X = [0.0, 0.5, 1.0]  # Left, Center, Right\n",
    "ALIGNMENT_Y = [0.0, 0.5, 1.0]  # Top, Center, Bottom\n",
    "\n",
    "# Interpolation for upscaling/downscaling\n",
    "# - cv2.INTER_LANCZOS4 is best in many cases for upscaling (but slower)\n",
    "# - cv2.INTER_CUBIC is also good for upscaling\n",
    "# - cv2.INTER_AREA is often better for downscaling\n",
    "UPSCALE_INTERPOLATION   = cv2.INTER_LANCZOS4\n",
    "DOWNSCALE_INTERPOLATION = cv2.INTER_AREA\n",
    "\n",
    "# ===============================================\n",
    "# FUNCTIONS\n",
    "# ===============================================\n",
    "\n",
    "def parse_labels(label_path):\n",
    "    \"\"\" Reads the label file and returns a dictionary: image_path -> list of bounding boxes. \"\"\"\n",
    "    labels = {}\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        image_path = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\"):\n",
    "                # New image path\n",
    "                image_path = line[2:]  # Remove '# '\n",
    "                labels[image_path] = []\n",
    "            else:\n",
    "                # bounding box in \"x_min y_min x_max y_max\" format\n",
    "                bbox = list(map(int, line.split()))\n",
    "                labels[image_path].append(bbox)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def scale_image(image, bboxes, scale):\n",
    "    \"\"\"\n",
    "    Scale the image by a certain factor.\n",
    "    Returns the scaled image and updated bounding boxes.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    # Ensure at least 1 pixel in width and height\n",
    "    new_w = int(orig_w * scale)\n",
    "    new_h = int(orig_h * scale)\n",
    "\n",
    "    if new_w < 1 or new_h < 1:\n",
    "        print(f\"Skipping scale {scale} for this image because dimension would be too small.\")\n",
    "        return image.copy(), []  # No bounding boxes\n",
    "\n",
    "    interp = DOWNSCALE_INTERPOLATION if scale < 1.0 else UPSCALE_INTERPOLATION\n",
    "    scaled_image = cv2.resize(image, (new_w, new_h), interpolation=interp)\n",
    "\n",
    "    # Scale bounding boxes\n",
    "    scaled_bboxes = []\n",
    "    for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "        x_min_s = int(x_min * (new_w / orig_w))\n",
    "        y_min_s = int(y_min * (new_h / orig_h))\n",
    "        x_max_s = int(x_max * (new_w / orig_w))\n",
    "        y_max_s = int(y_max * (new_h / orig_h))\n",
    "\n",
    "        # Filter invalid bounding boxes\n",
    "        if x_max_s > x_min_s and y_max_s > y_min_s:\n",
    "            scaled_bboxes.append([x_min_s, y_min_s, x_max_s, y_max_s])\n",
    "        else:\n",
    "            print(f\"Skipping invalid box after scaling: {[x_min_s, y_min_s, x_max_s, y_max_s]}\")\n",
    "\n",
    "    return scaled_image, scaled_bboxes\n",
    "\n",
    "\n",
    "def pad_image(image, bboxes, align_x=0.5, align_y=0.5):\n",
    "    \"\"\"\n",
    "    Pads the image to the target resolution (1920x1080).\n",
    "    Returns the padded image and updated bounding boxes.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "    pad_x = max(0, TARGET_WIDTH - orig_w)\n",
    "    pad_y = max(0, TARGET_HEIGHT - orig_h)\n",
    "\n",
    "    top = int(pad_y * align_y)\n",
    "    bottom = pad_y - top\n",
    "    left = int(pad_x * align_x)\n",
    "    right = pad_x - left\n",
    "\n",
    "    # Create a noise patch\n",
    "    noise = np.random.randint(0, 256, (TARGET_HEIGHT, TARGET_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "    # Place the original image in the center with padding filled with noise\n",
    "    padded_image = noise.copy()\n",
    "    padded_image[top:top+orig_h, left:left+orig_w] = image\n",
    "\n",
    "\n",
    "    # padded_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "    updated_bboxes = []\n",
    "    for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "        x_min_p = x_min + left\n",
    "        y_min_p = y_min + top\n",
    "        x_max_p = x_max + left\n",
    "        y_max_p = y_max + top\n",
    "\n",
    "        # Filter invalid bounding boxes\n",
    "        if x_max_p > x_min_p and y_max_p > y_min_p:\n",
    "            updated_bboxes.append([x_min_p, y_min_p, x_max_p, y_max_p])\n",
    "        else:\n",
    "            print(f\"Skipping invalid box after padding: {[x_min_p, y_min_p, x_max_p, y_max_p]}\")\n",
    "\n",
    "    return padded_image, updated_bboxes\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN PROCESS\n",
    "# ===============================================\n",
    "\n",
    "# 1) Parse labels\n",
    "labels_dict = parse_labels(labels_path)\n",
    "\n",
    "# 2) Create output folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 3) Calculate total steps for the progress bar\n",
    "num_images   = len(labels_dict)\n",
    "num_scales   = len(SCALES)\n",
    "num_positions = len(ALIGNMENT_X) * len(ALIGNMENT_Y)\n",
    "total_steps = num_images * num_scales * num_positions\n",
    "\n",
    "# 4) Open one single text file for all augmented results\n",
    "with open(output_label_file, 'w') as label_out, tqdm(total=total_steps, desc=\"Processing\") as pbar:\n",
    "    # 5) Iterate over images\n",
    "    for rel_img_path, bboxes in labels_dict.items():\n",
    "        full_img_path = os.path.join(dataset_path, rel_img_path)\n",
    "        if not os.path.isfile(full_img_path):\n",
    "            print(f\"[Warning] Image not found: {full_img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(full_img_path)\n",
    "        if image is None:\n",
    "            print(f\"[Warning] Failed to load: {full_img_path}\")\n",
    "            continue\n",
    "\n",
    "        # 6) For each scale\n",
    "        for scale in SCALES:\n",
    "            scaled_img, scaled_bboxes = scale_image(image, bboxes, scale)\n",
    "            if not scaled_bboxes:  # Skip if no valid boxes\n",
    "                continue\n",
    "\n",
    "            # 7) For each alignment\n",
    "            for ax in ALIGNMENT_X:\n",
    "                for ay in ALIGNMENT_Y:\n",
    "                    padded_img, padded_bboxes = pad_image(scaled_img, scaled_bboxes, align_x=ax, align_y=ay)\n",
    "                    if not padded_bboxes:  # Skip if no valid boxes\n",
    "                        continue\n",
    "\n",
    "                    # Create a file name for the augmented image\n",
    "                    base_name = os.path.splitext(os.path.basename(rel_img_path))[0]\n",
    "                    out_name  = f\"{base_name}_s{scale}_ax{ax}_ay{ay}.jpg\"\n",
    "\n",
    "                    # Save the augmented image\n",
    "                    out_path = os.path.join(output_dir, out_name)\n",
    "                    cv2.imwrite(out_path, padded_img)\n",
    "\n",
    "                    # ======================================\n",
    "                    # Write to single annotation file\n",
    "                    # ======================================\n",
    "                    label_out.write(f\"# {out_name}\\n\")\n",
    "                    for (x_min, y_min, x_max, y_max) in padded_bboxes:\n",
    "                        label_out.write(f\"{x_min} {y_min} {x_max} {y_max}\\n\")\n",
    "\n",
    "                    pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images_folder, labels_file, transform = None):\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform # technically should be a parameter, but due to situation, we are doing that seperately beforehand\n",
    "        self.data = []\n",
    "\n",
    "        with open(labels_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            image_path = None\n",
    "            boxes = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"#\"):\n",
    "                    if image_path:  # Save previous image\n",
    "                        self.data.append((image_path, boxes))\n",
    "                    image_path = os.path.join(images_folder, line[2:])\n",
    "                    boxes = []\n",
    "                else:\n",
    "                    boxes.append(list(map(int, line.split())))\n",
    "            if image_path:  # Save last image\n",
    "                self.data.append((image_path, boxes))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, boxes = self.data[idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = Image.open(image).convert('RGB')\n",
    "\n",
    "        # keep in for easy refactor in the future\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # # Convert to PyTorch tensor\n",
    "        image = torch.from_numpy(image).float().permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Convert bounding boxes to tensor\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Labels (assuming all objects belong to class 1, since it's face detection)\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102060\n"
     ]
    }
   ],
   "source": [
    "full_images_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\"\n",
    "full_labels_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\\all_augmentations.txt\"\n",
    "dataset = FaceDataset(full_images_path, full_labels_path)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, ssdlite\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "import math\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SSD300 with VGG16 backbone\n",
    "model = ssdlite320_mobilenet_v3_large(num_classes = 2)  # Set to True if you want pretrained weights\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Live plotting function\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(eval_losses, label=\"Eval Loss\", marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "prev_eval = math.inf\n",
    "# Training Loop with tqdm & Live Graph\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        # tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU if available\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "            # Compute loss\n",
    "            # loss = criterion(outputs, targets)\n",
    "            loss = sum(loss for loss in outputs.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate Model After Each Epoch\n",
    "        avg_eval_loss = evaluate_model(model, test_loader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "\n",
    "        if avg_eval_loss < prev_eval:\n",
    "            torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model_multi.pth\")\n",
    "\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function with tqdm Progress Bar\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # When targets are provided, the model returns a dictionary of losses.\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/20412 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 20412/20412 [53:27<00:00,  6.36batch/s, loss=4.5214]  \n",
      "Evaluating: 100%|██████████| 5103/5103 [11:29<00:00,  7.40batch/s, loss=4.7551] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss = 3.9376, Eval Loss = 3.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 20412/20412 [42:53<00:00,  7.93batch/s, loss=5.3979] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:18<00:00, 11.63batch/s, loss=4.9765] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss = 3.8551, Eval Loss = 3.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 20412/20412 [42:46<00:00,  7.95batch/s, loss=1.7779] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:17<00:00, 11.66batch/s, loss=4.5458] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss = 3.7828, Eval Loss = 3.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 20412/20412 [42:47<00:00,  7.95batch/s, loss=3.0150] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:20<00:00, 11.58batch/s, loss=4.8896] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss = 3.7144, Eval Loss = 3.7638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 20412/20412 [43:24<00:00,  7.84batch/s, loss=6.1099] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:22<00:00, 11.54batch/s, loss=4.8664] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss = 3.6605, Eval Loss = 3.7241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 20412/20412 [45:18<00:00,  7.51batch/s, loss=3.0254]  \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:34<00:00, 11.22batch/s, loss=4.9321] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss = 3.6145, Eval Loss = 3.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 20412/20412 [43:37<00:00,  7.80batch/s, loss=2.6094] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:18<00:00, 11.64batch/s, loss=4.7904] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss = 3.5592, Eval Loss = 3.6632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 20412/20412 [43:15<00:00,  7.86batch/s, loss=3.3826] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:18<00:00, 11.64batch/s, loss=4.4350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss = 3.5130, Eval Loss = 3.6112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 20412/20412 [45:49<00:00,  7.43batch/s, loss=3.6943] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:17<00:00, 11.66batch/s, loss=4.4635] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss = 3.4724, Eval Loss = 3.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 20412/20412 [44:31<00:00,  7.64batch/s, loss=1.0102] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:25<00:00, 11.45batch/s, loss=4.0743] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss = 3.4370, Eval Loss = 3.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 20412/20412 [44:01<00:00,  7.73batch/s, loss=3.8050] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:25<00:00, 11.47batch/s, loss=4.3079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss = 3.3957, Eval Loss = 3.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 20412/20412 [43:27<00:00,  7.83batch/s, loss=2.0283] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:16<00:00, 11.68batch/s, loss=4.3522] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss = 3.3672, Eval Loss = 3.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 20412/20412 [43:58<00:00,  7.74batch/s, loss=4.8387] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:22<00:00, 11.54batch/s, loss=4.3936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss = 3.3288, Eval Loss = 3.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 20412/20412 [43:19<00:00,  7.85batch/s, loss=5.1795] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:25<00:00, 11.45batch/s, loss=4.4363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss = 3.3003, Eval Loss = 3.5134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 20412/20412 [43:37<00:00,  7.80batch/s, loss=2.6226] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:18<00:00, 11.63batch/s, loss=3.7372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss = 3.2695, Eval Loss = 3.4835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 20412/20412 [43:42<00:00,  7.78batch/s, loss=5.4271] \n",
      "Evaluating: 100%|██████████| 5103/5103 [07:26<00:00, 11.42batch/s, loss=4.8075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss = 3.2427, Eval Loss = 3.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  31%|███▏      | 6398/20412 [15:37<34:12,  6.83batch/s, loss=1.9980]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train Model with Evaluation Each Epoch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_model(model, train_loader, test_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      8\u001b[0m plot_losses()\n",
      "Cell \u001b[1;32mIn[22], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# loss = criterion(outputs, targets)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "# Train Model with Evaluation Each Epoch\n",
    "train_model(model, train_loader, test_loader, num_epochs=20)\n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSDLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, ssdlite\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "import math\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SSD300 with VGG16 backbone\n",
    "model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)  # Set to True if you want pretrained weights\n",
    "# model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_channels = [list(m.parameters())[0].shape[0] for m in model.head.classification_head.module_list]\n",
    "\n",
    "# # Check input channels\n",
    "# num_anchors = model.head.classification_head.module_list[0][1].out_channels // 91  # Default COCO classes is 91\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 1080, 1920)\n",
    "# features = model.backbone(dummy_input)\n",
    "# feature_map_shapes = [f.shape[-2:] for f in features.values()]\n",
    "\n",
    "\n",
    "# # For example, using the height (1080) as the reference:\n",
    "# new_anchor_generator = get_new_anchor_generator(1080, feature_map_shapes)\n",
    "# # Replace the model's anchor generator:\n",
    "# model.anchor_generator = new_anchor_generator\n",
    "\n",
    "\n",
    "# # Modify classification head to have 2 classes \n",
    "# model.head.classification_head.num_classes = 2  # Update class count\n",
    "# model.head.classification_head = ssdlite.SSDLiteClassificationHead(\n",
    "#     in_channels = in_channels,\n",
    "#     num_anchors = [num_anchors], \n",
    "#     num_classes = 2,\n",
    "#     norm_layer = nn.BatchNorm2d\n",
    "# )\n",
    "\n",
    "# Compute the number of feature maps from the backbone\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920)\n",
    "features = model.backbone(dummy_input)\n",
    "feature_map_shapes = [f.shape[-2:] for f in features.values()]\n",
    "num_feature_maps = len(feature_map_shapes)\n",
    "\n",
    "# Define aspect ratios (one per feature map)\n",
    "aspect_ratios = [[1.0, 2.0, 0.5]] * num_feature_maps\n",
    "\n",
    "# Compute the number of anchors per feature map:\n",
    "# (Typically, it's len(aspect_ratios[i]) + 1 for the extra anchor)\n",
    "num_anchors_list = [len(ratios) + 1 for ratios in aspect_ratios]  # e.g. [4, 4, ..., 4]\n",
    "\n",
    "def get_new_anchor_generator(input_size, feature_map_shapes, aspect_ratios=None):\n",
    "    num_feature_maps = len(feature_map_shapes)\n",
    "    s_min, s_max = 0.2, 0.9\n",
    "    scales = [s_min + (s_max - s_min) * k / num_feature_maps for k in range(num_feature_maps + 1)]\n",
    "    \n",
    "    if aspect_ratios is None:\n",
    "        aspect_ratios = [[1.0, 2.0, 0.5]] * num_feature_maps\n",
    "    \n",
    "    anchor_generator = DefaultBoxGenerator(aspect_ratios, scales=scales)\n",
    "    return anchor_generator\n",
    "\n",
    "new_anchor_generator = get_new_anchor_generator(1080, feature_map_shapes, aspect_ratios=aspect_ratios)\n",
    "model.anchor_generator = new_anchor_generator\n",
    "\n",
    "# Compute the in_channels for each feature map head as before:\n",
    "in_channels = [list(m.parameters())[0].shape[0] for m in model.head.classification_head.module_list]\n",
    "\n",
    "model.head.classification_head = ssdlite.SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors_list,  # now a list for each feature map\n",
    "    num_classes=2,\n",
    "    norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "\n",
    "model.head.regression_head = ssdlite.SSDLiteRegressionHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors_list,\n",
    "    norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "\n",
    "\n",
    "# Modify for 1080p Input\n",
    "model.size = (1080, 1920)\n",
    "\n",
    "# Freeze all layers by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the first few layers of the backbone\n",
    "# for layer in list(model.backbone.features)[:1]:  # Modify the number as needed\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last layers of the detection head (classification + box regression)\n",
    "for param in model.head.classification_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.head.regression_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # Print which layers are trainable\n",
    "# trainable_layers = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "# print(\"Trainable layers:\", trainable_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Live plotting function\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(eval_losses, label=\"Eval Loss\", marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Training Loop with tqdm & Live Graph\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        # tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU if available\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "            # Compute loss\n",
    "            # loss = criterion(outputs, targets)\n",
    "            loss = sum(loss for loss in outputs.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate Model After Each Epoch\n",
    "        avg_eval_loss = evaluate_model(model, test_loader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function with tqdm Progress Bar\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # When targets are provided, the model returns a dictionary of losses.\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "for _ in range(1):\n",
    "    # Train Model with Evaluation Each Epoch\n",
    "    train_model(model, train_loader, test_loader, num_epochs=5)\n",
    "    plot_losses()\n",
    "    torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ONNX with 1080p Input\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920).to(device)  # Adjusted for 1080p\n",
    "torch.onnx.export(model, dummy_input, [\"ssd_1080p.onnx\"], dynamo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
