{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python tqdm numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDDB Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 102060/102060 [11:41<00:00, 145.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# CONFIG\n",
    "# ===============================================\n",
    "\n",
    "# Paths\n",
    "dataset_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\images\"\n",
    "labels_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\label.txt\"\n",
    "output_dir  = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\"  # Where to save results\n",
    "\n",
    "output_label_file = os.path.join(output_dir, \"all_augmentations.txt\")\n",
    "\n",
    "# Target resolution\n",
    "TARGET_HEIGHT, TARGET_WIDTH = 1080, 1920\n",
    "\n",
    "# Define a list of scales (feel free to adjust)\n",
    "SCALES = [0.5, 1.0]\n",
    "SCALES.extend([1/scale for scale in SCALES])\n",
    "\n",
    "# Define alignment anchors for both axes\n",
    "# e.g. 0 → align to the \"top\" or \"left\", 0.5 → align \"center\", 1.0 → align \"bottom\" or \"right\".\n",
    "# You could also do 0, 0.25, 0.5, 0.75, 1.0 to get 5 steps for each axis.\n",
    "ALIGNMENT_X = [0.0, 0.5, 1.0]  # Left, Center, Right\n",
    "ALIGNMENT_Y = [0.0, 0.5, 1.0]  # Top, Center, Bottom\n",
    "\n",
    "# Interpolation for upscaling/downscaling\n",
    "# - cv2.INTER_LANCZOS4 is best in many cases for upscaling (but slower)\n",
    "# - cv2.INTER_CUBIC is also good for upscaling\n",
    "# - cv2.INTER_AREA is often better for downscaling\n",
    "UPSCALE_INTERPOLATION   = cv2.INTER_LANCZOS4\n",
    "DOWNSCALE_INTERPOLATION = cv2.INTER_AREA\n",
    "\n",
    "# ===============================================\n",
    "# FUNCTIONS\n",
    "# ===============================================\n",
    "\n",
    "def parse_labels(label_path):\n",
    "    \"\"\" Reads the label file and returns a dictionary: image_path -> list of bounding boxes. \"\"\"\n",
    "    labels = {}\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        image_path = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\"):\n",
    "                # New image path\n",
    "                image_path = line[2:]  # Remove '# '\n",
    "                labels[image_path] = []\n",
    "            else:\n",
    "                # bounding box in \"x_min y_min x_max y_max\" format\n",
    "                bbox = list(map(int, line.split()))\n",
    "                labels[image_path].append(bbox)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def scale_image(image, bboxes, scale):\n",
    "    \"\"\"\n",
    "    Scale the image by a certain factor.\n",
    "    Returns the scaled image and updated bounding boxes.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    # Ensure at least 1 pixel in width and height\n",
    "    new_w = int(orig_w * scale)\n",
    "    new_h = int(orig_h * scale)\n",
    "\n",
    "    if new_w < 1 or new_h < 1:\n",
    "        print(f\"Skipping scale {scale} for this image because dimension would be too small.\")\n",
    "        return image.copy(), bboxes.copy()  # or just skip\n",
    "\n",
    "    \n",
    "    if scale < 1.0:\n",
    "        # Downscale => INTER_AREA is typically good for reducing sizes\n",
    "        interp = DOWNSCALE_INTERPOLATION\n",
    "    else:\n",
    "        # Upscale => e.g., INTER_LANCZOS4 or INTER_CUBIC\n",
    "        interp = UPSCALE_INTERPOLATION\n",
    "    \n",
    "    # Resize with safe dimension sizes\n",
    "    scaled_image = cv2.resize(image, (new_w, new_h), interpolation=interp)\n",
    "    \n",
    "    # Scale bounding boxes\n",
    "    scaled_bboxes = []\n",
    "    for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "        x_min_s = int(x_min * (new_w / orig_w))\n",
    "        y_min_s = int(y_min * (new_h / orig_h))\n",
    "        x_max_s = int(x_max * (new_w / orig_w))\n",
    "        y_max_s = int(y_max * (new_h / orig_h))\n",
    "        scaled_bboxes.append([x_min_s, y_min_s, x_max_s, y_max_s])\n",
    "    \n",
    "    return scaled_image, scaled_bboxes\n",
    "\n",
    "\n",
    "def pad_image(image, bboxes, align_x=0.5, align_y=0.5):\n",
    "    \"\"\"\n",
    "    Pads the image to the target resolution (1920x1080).\n",
    "    `align_x` and `align_y` ∈ [0, 1], controlling the alignment along each axis:\n",
    "        - 0.0 => flush to top/left\n",
    "        - 0.5 => center\n",
    "        - 1.0 => flush to bottom/right\n",
    "    Returns the padded image and updated bounding boxes.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    # How much total padding is needed on each dimension\n",
    "    pad_x = max(0, TARGET_WIDTH - orig_w)\n",
    "    pad_y = max(0, TARGET_HEIGHT - orig_h)\n",
    "    \n",
    "    # Compute how much goes on the top vs bottom\n",
    "    # e.g. if align_y=0, all the padding goes to bottom\n",
    "    #      if align_y=1, all the padding goes to top\n",
    "    #      if align_y=0.5, half top, half bottom\n",
    "    top = int(pad_y * align_y)\n",
    "    bottom = pad_y - top\n",
    "    \n",
    "    # Similarly for left, right\n",
    "    left = int(pad_x * align_x)\n",
    "    right = pad_x - left\n",
    "    \n",
    "    # Pad the image\n",
    "    padded_image = cv2.copyMakeBorder(\n",
    "        image,\n",
    "        top, bottom, left, right,\n",
    "        cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    \n",
    "    # Shift bounding boxes\n",
    "    updated_bboxes = []\n",
    "    for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "        x_min_pad = x_min + left\n",
    "        x_max_pad = x_max + left\n",
    "        y_min_pad = y_min + top\n",
    "        y_max_pad = y_max + top\n",
    "        updated_bboxes.append([x_min_pad, y_min_pad, x_max_pad, y_max_pad])\n",
    "    \n",
    "    return padded_image, updated_bboxes\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN PROCESS\n",
    "# ===============================================\n",
    "\n",
    "# 1) Parse labels\n",
    "labels_dict = parse_labels(labels_path)\n",
    "\n",
    "# 2) Create output folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 3) Calculate total steps for the progress bar\n",
    "num_images   = len(labels_dict)\n",
    "num_scales   = len(SCALES)\n",
    "num_positions = len(ALIGNMENT_X) * len(ALIGNMENT_Y)\n",
    "total_steps = num_images * num_scales * num_positions\n",
    "\n",
    "# 4) Open one single text file for all augmented results\n",
    "with open(output_label_file, 'w') as label_out, tqdm(total=total_steps, desc=\"Processing\") as pbar:\n",
    "\n",
    "    # 5) Iterate over images\n",
    "    for rel_img_path, bboxes in labels_dict.items():\n",
    "        full_img_path = os.path.join(dataset_path, rel_img_path)\n",
    "        if not os.path.isfile(full_img_path):\n",
    "            print(f\"[Warning] Image not found: {full_img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(full_img_path)\n",
    "        if image is None:\n",
    "            print(f\"[Warning] Failed to load: {full_img_path}\")\n",
    "            continue\n",
    "\n",
    "        # 6) For each scale\n",
    "        for scale in SCALES:\n",
    "            scaled_img, scaled_bboxes = scale_image(image, bboxes, scale)\n",
    "\n",
    "            # 7) For each alignment\n",
    "            for ax in ALIGNMENT_X:\n",
    "                for ay in ALIGNMENT_Y:\n",
    "                    padded_img, padded_bboxes = pad_image(scaled_img, scaled_bboxes,\n",
    "                                                            align_x=ax, align_y=ay)\n",
    "\n",
    "                    # Create a file name for the augmented image\n",
    "                    base_name = os.path.splitext(os.path.basename(rel_img_path))[0]\n",
    "                    out_name  = f\"{base_name}_s{scale}_ax{ax}_ay{ay}.jpg\"\n",
    "\n",
    "                    # Save the augmented image (comment out if not needed)\n",
    "                    out_path = os.path.join(output_dir, out_name)\n",
    "                    cv2.imwrite(out_path, padded_img)\n",
    "\n",
    "                    # ======================================\n",
    "                    # Write to single annotation file\n",
    "                    # ======================================\n",
    "                    # 1) Write a header line for the new image version\n",
    "                    label_out.write(f\"# {out_name}\\n\")\n",
    "\n",
    "                    # 2) Write each bounding box on its own line\n",
    "                    for (x_min, y_min, x_max, y_max) in padded_bboxes:\n",
    "                        label_out.write(f\"{x_min} {y_min} {x_max} {y_max}\\n\")\n",
    "\n",
    "                    pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images_folder, labels_file, transform = None):\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform # technically should be a parameter, but due to situation, we are doing that seperately beforehand\n",
    "        self.data = []\n",
    "\n",
    "        with open(labels_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            image_path = None\n",
    "            boxes = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"#\"):\n",
    "                    if image_path:  # Save previous image\n",
    "                        self.data.append((image_path, boxes))\n",
    "                    image_path = os.path.join(images_folder, line[2:])\n",
    "                    boxes = []\n",
    "                else:\n",
    "                    boxes.append(list(map(int, line.split())))\n",
    "            if image_path:  # Save last image\n",
    "                self.data.append((image_path, boxes))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, boxes = self.data[idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = Image.open(image).convert('RGB')\n",
    "\n",
    "        # keep in for easy refactor in the future\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # # Convert to PyTorch tensor\n",
    "        image = torch.from_numpy(image).float().permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Convert bounding boxes to tensor\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Labels (assuming all objects belong to class 1, since it's face detection)\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_images_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\aug_images\"\n",
    "full_labels_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\aug_label.txt\"\n",
    "dataset = FaceDataset(full_images_path, full_labels_path)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, ssdlite\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "import math\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SSD300 with VGG16 backbone\n",
    "model = ssdlite320_mobilenet_v3_large(num_classes = 2)  # Set to True if you want pretrained weights\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Live plotting function\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(eval_losses, label=\"Eval Loss\", marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "prev_eval = math.inf\n",
    "# Training Loop with tqdm & Live Graph\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        # tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU if available\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "            # Compute loss\n",
    "            # loss = criterion(outputs, targets)\n",
    "            loss = sum(loss for loss in outputs.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate Model After Each Epoch\n",
    "        avg_eval_loss = evaluate_model(model, test_loader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "\n",
    "        if avg_eval_loss < prev_eval:\n",
    "            torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model_default.pth\")\n",
    "\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function with tqdm Progress Bar\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # When targets are provided, the model returns a dictionary of losses.\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 5103/5103 [12:12<00:00,  6.97batch/s, loss=2.6525]\n",
      "Evaluating: 100%|██████████| 1276/1276 [02:25<00:00,  8.77batch/s, loss=4.8882] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss = 2.7857, Eval Loss = 3.9285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 5103/5103 [10:59<00:00,  7.74batch/s, loss=3.7874]\n",
      "Evaluating: 100%|██████████| 1276/1276 [01:48<00:00, 11.72batch/s, loss=5.2102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss = 2.6984, Eval Loss = 3.9426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 5103/5103 [10:47<00:00,  7.89batch/s, loss=2.2836]\n",
      "Evaluating: 100%|██████████| 1276/1276 [01:49<00:00, 11.66batch/s, loss=5.2013] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss = 2.6537, Eval Loss = 3.9631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 5103/5103 [10:41<00:00,  7.96batch/s, loss=4.5308]\n",
      "Evaluating: 100%|██████████| 1276/1276 [01:49<00:00, 11.62batch/s, loss=5.2242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss = 2.6237, Eval Loss = 3.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 5103/5103 [10:40<00:00,  7.97batch/s, loss=4.6423]\n",
      "Evaluating: 100%|██████████| 1276/1276 [01:49<00:00, 11.66batch/s, loss=5.1551] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss = 2.6098, Eval Loss = 3.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  56%|█████▋    | 2883/5103 [08:00<04:36,  8.03batch/s, loss=1.5773]"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "# Train Model with Evaluation Each Epoch\n",
    "train_model(model, train_loader, test_loader, num_epochs=20)\n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSDLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, ssdlite\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "import math\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SSD300 with VGG16 backbone\n",
    "model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)  # Set to True if you want pretrained weights\n",
    "# model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_channels = [list(m.parameters())[0].shape[0] for m in model.head.classification_head.module_list]\n",
    "\n",
    "# # Check input channels\n",
    "# num_anchors = model.head.classification_head.module_list[0][1].out_channels // 91  # Default COCO classes is 91\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 1080, 1920)\n",
    "# features = model.backbone(dummy_input)\n",
    "# feature_map_shapes = [f.shape[-2:] for f in features.values()]\n",
    "\n",
    "\n",
    "# # For example, using the height (1080) as the reference:\n",
    "# new_anchor_generator = get_new_anchor_generator(1080, feature_map_shapes)\n",
    "# # Replace the model's anchor generator:\n",
    "# model.anchor_generator = new_anchor_generator\n",
    "\n",
    "\n",
    "# # Modify classification head to have 2 classes \n",
    "# model.head.classification_head.num_classes = 2  # Update class count\n",
    "# model.head.classification_head = ssdlite.SSDLiteClassificationHead(\n",
    "#     in_channels = in_channels,\n",
    "#     num_anchors = [num_anchors], \n",
    "#     num_classes = 2,\n",
    "#     norm_layer = nn.BatchNorm2d\n",
    "# )\n",
    "\n",
    "# Compute the number of feature maps from the backbone\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920)\n",
    "features = model.backbone(dummy_input)\n",
    "feature_map_shapes = [f.shape[-2:] for f in features.values()]\n",
    "num_feature_maps = len(feature_map_shapes)\n",
    "\n",
    "# Define aspect ratios (one per feature map)\n",
    "aspect_ratios = [[1.0, 2.0, 0.5]] * num_feature_maps\n",
    "\n",
    "# Compute the number of anchors per feature map:\n",
    "# (Typically, it's len(aspect_ratios[i]) + 1 for the extra anchor)\n",
    "num_anchors_list = [len(ratios) + 1 for ratios in aspect_ratios]  # e.g. [4, 4, ..., 4]\n",
    "\n",
    "def get_new_anchor_generator(input_size, feature_map_shapes, aspect_ratios=None):\n",
    "    num_feature_maps = len(feature_map_shapes)\n",
    "    s_min, s_max = 0.2, 0.9\n",
    "    scales = [s_min + (s_max - s_min) * k / num_feature_maps for k in range(num_feature_maps + 1)]\n",
    "    \n",
    "    if aspect_ratios is None:\n",
    "        aspect_ratios = [[1.0, 2.0, 0.5]] * num_feature_maps\n",
    "    \n",
    "    anchor_generator = DefaultBoxGenerator(aspect_ratios, scales=scales)\n",
    "    return anchor_generator\n",
    "\n",
    "new_anchor_generator = get_new_anchor_generator(1080, feature_map_shapes, aspect_ratios=aspect_ratios)\n",
    "model.anchor_generator = new_anchor_generator\n",
    "\n",
    "# Compute the in_channels for each feature map head as before:\n",
    "in_channels = [list(m.parameters())[0].shape[0] for m in model.head.classification_head.module_list]\n",
    "\n",
    "model.head.classification_head = ssdlite.SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors_list,  # now a list for each feature map\n",
    "    num_classes=2,\n",
    "    norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "\n",
    "model.head.regression_head = ssdlite.SSDLiteRegressionHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors_list,\n",
    "    norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "\n",
    "\n",
    "# Modify for 1080p Input\n",
    "model.size = (1080, 1920)\n",
    "\n",
    "# Freeze all layers by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the first few layers of the backbone\n",
    "# for layer in list(model.backbone.features)[:1]:  # Modify the number as needed\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last layers of the detection head (classification + box regression)\n",
    "for param in model.head.classification_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.head.regression_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # Print which layers are trainable\n",
    "# trainable_layers = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "# print(\"Trainable layers:\", trainable_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Live plotting function\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(eval_losses, label=\"Eval Loss\", marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Training Loop with tqdm & Live Graph\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        # tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU if available\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "            # Compute loss\n",
    "            # loss = criterion(outputs, targets)\n",
    "            loss = sum(loss for loss in outputs.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate Model After Each Epoch\n",
    "        avg_eval_loss = evaluate_model(model, test_loader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function with tqdm Progress Bar\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # When targets are provided, the model returns a dictionary of losses.\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "for _ in range(1):\n",
    "    # Train Model with Evaluation Each Epoch\n",
    "    train_model(model, train_loader, test_loader, num_epochs=5)\n",
    "    plot_losses()\n",
    "    torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ONNX with 1080p Input\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920).to(device)  # Adjusted for 1080p\n",
    "torch.onnx.export(model, dummy_input, [\"ssd_1080p.onnx\"], dynamo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
