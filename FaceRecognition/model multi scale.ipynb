{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python tqdm numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDDB Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================================\n",
    "# CONFIG\n",
    "# ===============================================\n",
    "\n",
    "dataset_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\images\"\n",
    "labels_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Dataset_FDDB\\label.txt\"\n",
    "output_dir  = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\"\n",
    "\n",
    "output_label_file = os.path.join(output_dir, \"all_augmentations.txt\")\n",
    "\n",
    "TARGET_HEIGHT, TARGET_WIDTH = 1080, 1920\n",
    "\n",
    "# Example: if you truly want rotations at 120° and 240°,\n",
    "# you need a custom rotation. Otherwise, keep [0, 180].\n",
    "ROTATIONS = [0, 120, 240]  \n",
    "\n",
    "SCALES = [0.5, 1.0, 1.5]\n",
    "ALIGNMENT_X = [0.0, 0.5, 1.0] #in theory we can have 2 instead of 3 if we really need to cut it down\n",
    "ALIGNMENT_Y = [0.0, 0.5, 1.0]\n",
    "NOISE_VARIANTS = [ \"gaussian\"] # \"none\", <- we can just have noisy images\n",
    "\n",
    "UPSCALE_INTERPOLATION   = cv2.INTER_LANCZOS4\n",
    "DOWNSCALE_INTERPOLATION = cv2.INTER_AREA\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ===============================================\n",
    "# FUNCTIONS\n",
    "# ===============================================\n",
    "\n",
    "def parse_labels(label_path):\n",
    "    \"\"\"\n",
    "    Reads the label file and returns a dict:\n",
    "       image_path -> list of bounding boxes [(x_min, y_min, x_max, y_max), ...]\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        current_image = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                # e.g. \"# 2003/07/19/img_123.jpg\"\n",
    "                current_image = line[2:].strip()\n",
    "                labels[current_image] = []\n",
    "            else:\n",
    "                bbox = list(map(int, line.split()))\n",
    "                labels[current_image].append(bbox)\n",
    "    return labels\n",
    "\n",
    "def adjust_brightness(image, factor):\n",
    "    \"\"\"\n",
    "    Adjusts brightness by scaling pixel values by 'factor'.\n",
    "    \"\"\"\n",
    "    # convert to float, scale, clip back to [0, 255], convert to uint8\n",
    "    out = image.astype(np.float32) * factor\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "def add_gaussian_noise(image, std=25):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise with standard deviation 'std'.\n",
    "    Returns a uint8 image with saturation/clip.\n",
    "    \"\"\"\n",
    "    # Convert to float for adding possibly negative values\n",
    "    float_image = image.astype(np.float32)\n",
    "    noise = np.random.normal(0, std, image.shape).astype(np.float32)\n",
    "    noisy = float_image + noise\n",
    "    noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotate 'image' by 'angle' degrees around its center, \n",
    "    returning a new image that fits the entire rotated result.\n",
    "    For angles other than 0, 90, 180, 270, the output shape grows\n",
    "    to avoid cropping corners.\n",
    "    \"\"\"\n",
    "    if angle % 360 == 0:\n",
    "        return image  # No rotation needed\n",
    "    \n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w / 2.0, h / 2.0)\n",
    "\n",
    "    # Rotation matrix\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "    # Compute the new bounding dimensions\n",
    "    cos_val = abs(M[0, 0])\n",
    "    sin_val = abs(M[0, 1])\n",
    "    new_w = int((h * sin_val) + (w * cos_val))\n",
    "    new_h = int((h * cos_val) + (w * sin_val))\n",
    "\n",
    "    # Adjust the rotation matrix to take into account translation\n",
    "    M[0, 2] += (new_w / 2.0) - center[0]\n",
    "    M[1, 2] += (new_h / 2.0) - center[1]\n",
    "\n",
    "    # Perform the rotation\n",
    "    rotated = cv2.warpAffine(image, M, (new_w, new_h))\n",
    "    return rotated\n",
    "\n",
    "def place_image_on_canvas(image, target_height, target_width, align_x, align_y):\n",
    "    \"\"\"\n",
    "    Places 'image' onto a black canvas of shape (target_height, target_width),\n",
    "    using alignment factors [0.0, 1.0] in x and y.\n",
    "\n",
    "    Returns:\n",
    "      canvas: the final image of shape (target_height, target_width, 3)\n",
    "      (offset_x, offset_y): where the top-left of 'image' was placed\n",
    "    \"\"\"\n",
    "    canvas = np.zeros((target_height, target_width, 3), dtype=np.uint8)\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Ensure we don't exceed the canvas if image is larger\n",
    "    # (You could also choose to resize if bigger, or skip)\n",
    "    if h > target_height or w > target_width:\n",
    "        # A simple fallback: fit it exactly by resizing\n",
    "        image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "    offset_x = int(align_x * (target_width - w))\n",
    "    offset_y = int(align_y * (target_height - h))\n",
    "\n",
    "    canvas[offset_y:offset_y+h, offset_x:offset_x+w] = image\n",
    "    return canvas, (offset_x, offset_y)\n",
    "\n",
    "def transform_bboxes(bboxes, scale, angle, offset_x, offset_y,\n",
    "                     original_w, original_h, final_w, final_h):\n",
    "    \"\"\"\n",
    "    Example of how you might transform bounding boxes for:\n",
    "      1) scaling\n",
    "      2) rotation (arbitrary angle)\n",
    "      3) offset (due to placement on the canvas)\n",
    "    \"\"\"\n",
    "    # 1) scale bboxes\n",
    "    scaled_bboxes = []\n",
    "    for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "        scaled_bboxes.append([\n",
    "            x_min * scale, \n",
    "            y_min * scale, \n",
    "            x_max * scale, \n",
    "            y_max * scale\n",
    "        ])\n",
    "\n",
    "    if angle % 360 == 0:\n",
    "        # Only offset\n",
    "        final_bboxes = []\n",
    "        for (sx_min, sy_min, sx_max, sy_max) in scaled_bboxes:\n",
    "            final_bboxes.append((\n",
    "                int(sx_min + offset_x), \n",
    "                int(sy_min + offset_y),\n",
    "                int(sx_max + offset_x),\n",
    "                int(sy_max + offset_y)\n",
    "            ))\n",
    "        return final_bboxes\n",
    "\n",
    "    # Build rotation matrix for 'angle' about the center of the scaled image\n",
    "    center_x = (scale * original_w) / 2.0\n",
    "    center_y = (scale * original_h) / 2.0\n",
    "    M = cv2.getRotationMatrix2D((center_x, center_y), angle, 1.0)\n",
    "\n",
    "    # Figure out new bounding dimension after rotation\n",
    "    cos_val = abs(M[0, 0])\n",
    "    sin_val = abs(M[0, 1])\n",
    "    new_w = int((scale * original_h) * sin_val + (scale * original_w) * cos_val)\n",
    "    new_h = int((scale * original_h) * cos_val + (scale * original_w) * sin_val)\n",
    "\n",
    "    # Adjust the rotation matrix for translation so the top-left corner is at (0,0)\n",
    "    M[0, 2] += (new_w / 2.0) - center_x\n",
    "    M[1, 2] += (new_h / 2.0) - center_y\n",
    "\n",
    "    final_bboxes = []\n",
    "    for (sx_min, sy_min, sx_max, sy_max) in scaled_bboxes:\n",
    "        # Convert to corner points\n",
    "        corners = np.array([\n",
    "            [sx_min, sy_min],\n",
    "            [sx_min, sy_max],\n",
    "            [sx_max, sy_min],\n",
    "            [sx_max, sy_max]\n",
    "        ], dtype=np.float32)\n",
    "        # Add 1 for the affine\n",
    "        ones = np.ones((4, 1), dtype=np.float32)\n",
    "        corners_ones = np.hstack([corners, ones])\n",
    "\n",
    "        # Transform corners\n",
    "        transformed = M @ corners_ones.T  # shape (2,4)\n",
    "\n",
    "        tx = transformed[0, :]\n",
    "        ty = transformed[1, :]\n",
    "        min_x, max_x = np.min(tx), np.max(tx)\n",
    "        min_y, max_y = np.min(ty), np.max(ty)\n",
    "\n",
    "        # Now offset for final placement on the canvas\n",
    "        final_xmin = int(min_x + offset_x)\n",
    "        final_ymin = int(min_y + offset_y)\n",
    "        final_xmax = int(max_x + offset_x)\n",
    "        final_ymax = int(max_y + offset_y)\n",
    "\n",
    "        # Clip to valid range if desired\n",
    "        final_xmin = max(0, min(final_xmin, final_w - 1))\n",
    "        final_ymin = max(0, min(final_ymin, final_h - 1))\n",
    "        final_xmax = max(0, min(final_xmax, final_w - 1))\n",
    "        final_ymax = max(0, min(final_ymax, final_h - 1))\n",
    "\n",
    "        final_bboxes.append((final_xmin, final_ymin, final_xmax, final_ymax))\n",
    "    \n",
    "    return final_bboxes\n",
    "\n",
    "def process_image(args):\n",
    "    \"\"\"\n",
    "    Processes a single image with the desired augmentations\n",
    "    and writes them to disk. Returns a list of (aug_filename, new_bboxes).\n",
    "    \"\"\"\n",
    "    image_path, bboxes = args\n",
    "    full_img_path = os.path.join(dataset_path, image_path)\n",
    "\n",
    "    image = cv2.imread(full_img_path)\n",
    "    if image is None:\n",
    "        # Skip missing or unreadable images\n",
    "        return []\n",
    "\n",
    "    original_h, original_w = image.shape[:2]\n",
    "    augmented_files = []\n",
    "\n",
    "    for scale in SCALES:\n",
    "        # Choose upscale vs. downscale interpolation\n",
    "        interp = UPSCALE_INTERPOLATION if scale >= 1.0 else DOWNSCALE_INTERPOLATION\n",
    "        scaled_img = cv2.resize(image, None, fx=scale, fy=scale, interpolation=interp)\n",
    "\n",
    "        for ax in ALIGNMENT_X:\n",
    "            for ay in ALIGNMENT_Y:\n",
    "                # Place scaled_img on the 1920x1080 canvas at alignment offsets\n",
    "                placed_img, (offset_x, offset_y) = place_image_on_canvas(\n",
    "                    scaled_img, TARGET_HEIGHT, TARGET_WIDTH, ax, ay\n",
    "                )\n",
    "\n",
    "                for angle in ROTATIONS:\n",
    "                    # Rotate the entire 1920x1080 canvas\n",
    "                    rotated_img = rotate_image(placed_img, angle)\n",
    "\n",
    "                    # For bounding-box transformations:\n",
    "                    # We transform them from original to final coords\n",
    "                    final_h, final_w = rotated_img.shape[:2]\n",
    "                    transformed_bboxes = transform_bboxes(\n",
    "                        bboxes,\n",
    "                        scale=scale,\n",
    "                        angle=angle,\n",
    "                        offset_x=offset_x,\n",
    "                        offset_y=offset_y,\n",
    "                        original_w=original_w,\n",
    "                        original_h=original_h,\n",
    "                        final_w=final_w,\n",
    "                        final_h=final_h\n",
    "                    )\n",
    "\n",
    "                    for noise_type in NOISE_VARIANTS:\n",
    "                        if noise_type == \"gaussian\":\n",
    "                            noisy_img = add_gaussian_noise(rotated_img)\n",
    "                        else:\n",
    "                            noisy_img = rotated_img\n",
    "\n",
    "                        # Different brightness variations\n",
    "                        for variation, factor in [ # (\"original\", 1.0), <- original is good, but the harder tasks will still teach it\n",
    "                                                  (\"shaded\", 0.5),\n",
    "                                                  (\"bright\", 1.5)]:\n",
    "                            final_img = adjust_brightness(noisy_img, factor)\n",
    "\n",
    "                            base_name = os.path.basename(os.path.splitext(image_path)[0])  # Get only the filename\n",
    "                            out_name = (\n",
    "                                f\"{base_name}\"\n",
    "                                f\"_s{scale}\"\n",
    "                                f\"_ax{ax}\"\n",
    "                                f\"_ay{ay}\"\n",
    "                                f\"_r{angle}\"\n",
    "                                f\"_n{noise_type}\"\n",
    "                                f\"_{variation}.jpg\"\n",
    "                            )\n",
    "                            out_path = os.path.join(output_dir, out_name)  # Ensures no subdirectories\n",
    "\n",
    "\n",
    "                            cv2.imwrite(out_path, final_img)\n",
    "                            # print(\"Finish Writing\" + str(out_path))\n",
    "                            augmented_files.append((out_name, transformed_bboxes))\n",
    "\n",
    "    return augmented_files\n",
    "\n",
    "# ===============================================\n",
    "# SINGLE-THREADED EXECUTION\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Label Parsing\")\n",
    "    labels_dict = parse_labels(labels_path)\n",
    "    print(\"Finish Label Parsing\")\n",
    "    # Convert dict to a list of (image_path, list_of_bboxes)\n",
    "    images_list = list(labels_dict.items())\n",
    "    images_list = images_list[:len(images_list)//10]\n",
    "\n",
    "    print(\"Start Image Augmentation\")\n",
    "    # Single-threaded loop with tqdm\n",
    "    all_results = []\n",
    "    for item in tqdm(images_list, desc=\"Processing\"):\n",
    "        aug_data = process_image(item)\n",
    "        all_results.append(aug_data)\n",
    "    print(\"Finish Image Augmentation\")\n",
    "    print(\"Start Box Writing\")\n",
    "    # Write bounding box annotations for all augmentations\n",
    "    with open(output_label_file, \"w\") as label_out:\n",
    "        for result in all_results:\n",
    "            for filename, bboxes in result:\n",
    "                label_out.write(f\"# {filename}\\n\")\n",
    "                for x_min, y_min, x_max, y_max in bboxes:\n",
    "                    label_out.write(f\"{x_min} {y_min} {x_max} {y_max}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images_folder, labels_file, transform = None):\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform # technically should be a parameter, but due to situation, we are doing that seperately beforehand\n",
    "        self.data = []\n",
    "\n",
    "        with open(labels_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            image_path = None\n",
    "            boxes = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"#\"):\n",
    "                    if image_path:  # Save previous image\n",
    "                        self.data.append((image_path, boxes))\n",
    "                    image_path = os.path.join(images_folder, line[2:])\n",
    "                    boxes = []\n",
    "                else:\n",
    "                    boxes.append(list(map(int, line.split())))\n",
    "            if image_path:  # Save last image\n",
    "                self.data.append((image_path, boxes))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, boxes = self.data[idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = Image.open(image).convert('RGB')\n",
    "\n",
    "        # keep in for easy refactor in the future\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # # Convert to PyTorch tensor\n",
    "        image = torch.from_numpy(image).float().permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        # Convert bounding boxes to tensor\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Labels (assuming all objects belong to class 1, since it's face detection)\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102060\n"
     ]
    }
   ],
   "source": [
    "full_images_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\"\n",
    "full_labels_path = r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\data\\Augmented\\all_augmentations.txt\"\n",
    "dataset = FaceDataset(full_images_path, full_labels_path)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, ssdlite\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "import math\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SSD300 with VGG16 backbone\n",
    "model = ssdlite320_mobilenet_v3_large(num_classes = 2)  # Set to True if you want pretrained weights\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Live plotting function\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(eval_losses, label=\"Eval Loss\", marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "prev_eval = math.inf\n",
    "# Training Loop with tqdm & Live Graph\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        # tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU if available\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "            # Compute loss\n",
    "            # loss = criterion(outputs, targets)\n",
    "            loss = sum(loss for loss in outputs.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate Model After Each Epoch\n",
    "        avg_eval_loss = evaluate_model(model, test_loader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "\n",
    "        if avg_eval_loss < prev_eval:\n",
    "            torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model_multi.pth\")\n",
    "\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function with tqdm Progress Bar\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # When targets are provided, the model returns a dictionary of losses.\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 20412/20412 [1:19:18<00:00,  4.29batch/s, loss=3.3220] \n",
      "Evaluating: 100%|██████████| 5103/5103 [18:35<00:00,  4.58batch/s, loss=2.8462] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss = 4.4925, Eval Loss = 4.1635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 20412/20412 [1:17:09<00:00,  4.41batch/s, loss=4.9906] \n",
      "Evaluating: 100%|██████████| 5103/5103 [22:48<00:00,  3.73batch/s, loss=2.6169] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss = 4.0726, Eval Loss = 3.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 20412/20412 [1:41:12<00:00,  3.36batch/s, loss=3.9306] \n",
      "Evaluating: 100%|██████████| 5103/5103 [21:09<00:00,  4.02batch/s, loss=2.9721] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss = 3.9190, Eval Loss = 3.8657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  73%|███████▎  | 14821/20412 [56:30<21:19,  4.37batch/s, loss=5.0928] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train Model with Evaluation Each Epoch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_model(model, train_loader, test_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      8\u001b[0m plot_losses()\n",
      "Cell \u001b[1;32mIn[33], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# loss = criterion(outputs, targets)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "# Train Model with Evaluation Each Epoch\n",
    "train_model(model, train_loader, test_loader, num_epochs=20)\n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSDLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, ssdlite\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "import math\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SSD300 with VGG16 backbone\n",
    "model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)  # Set to True if you want pretrained weights\n",
    "# model.to(device)\n",
    "\n",
    "# Get the number of input features for the classification head\n",
    "in_channels = [list(m.parameters())[0].shape[0] for m in model.head.classification_head.module_list]\n",
    "\n",
    "# # Check input channels\n",
    "# num_anchors = model.head.classification_head.module_list[0][1].out_channels // 91  # Default COCO classes is 91\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 1080, 1920)\n",
    "# features = model.backbone(dummy_input)\n",
    "# feature_map_shapes = [f.shape[-2:] for f in features.values()]\n",
    "\n",
    "\n",
    "# # For example, using the height (1080) as the reference:\n",
    "# new_anchor_generator = get_new_anchor_generator(1080, feature_map_shapes)\n",
    "# # Replace the model's anchor generator:\n",
    "# model.anchor_generator = new_anchor_generator\n",
    "\n",
    "\n",
    "# # Modify classification head to have 2 classes \n",
    "# model.head.classification_head.num_classes = 2  # Update class count\n",
    "# model.head.classification_head = ssdlite.SSDLiteClassificationHead(\n",
    "#     in_channels = in_channels,\n",
    "#     num_anchors = [num_anchors], \n",
    "#     num_classes = 2,\n",
    "#     norm_layer = nn.BatchNorm2d\n",
    "# )\n",
    "\n",
    "# Compute the number of feature maps from the backbone\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920)\n",
    "features = model.backbone(dummy_input)\n",
    "feature_map_shapes = [f.shape[-2:] for f in features.values()]\n",
    "num_feature_maps = len(feature_map_shapes)\n",
    "\n",
    "# Define aspect ratios (one per feature map)\n",
    "aspect_ratios = [[1.0, 2.0, 0.5]] * num_feature_maps\n",
    "\n",
    "# Compute the number of anchors per feature map:\n",
    "# (Typically, it's len(aspect_ratios[i]) + 1 for the extra anchor)\n",
    "num_anchors_list = [len(ratios) + 1 for ratios in aspect_ratios]  # e.g. [4, 4, ..., 4]\n",
    "\n",
    "def get_new_anchor_generator(input_size, feature_map_shapes, aspect_ratios=None):\n",
    "    num_feature_maps = len(feature_map_shapes)\n",
    "    s_min, s_max = 0.2, 0.9\n",
    "    scales = [s_min + (s_max - s_min) * k / num_feature_maps for k in range(num_feature_maps + 1)]\n",
    "    \n",
    "    if aspect_ratios is None:\n",
    "        aspect_ratios = [[1.0, 2.0, 0.5]] * num_feature_maps\n",
    "    \n",
    "    anchor_generator = DefaultBoxGenerator(aspect_ratios, scales=scales)\n",
    "    return anchor_generator\n",
    "\n",
    "new_anchor_generator = get_new_anchor_generator(1080, feature_map_shapes, aspect_ratios=aspect_ratios)\n",
    "model.anchor_generator = new_anchor_generator\n",
    "\n",
    "# Compute the in_channels for each feature map head as before:\n",
    "in_channels = [list(m.parameters())[0].shape[0] for m in model.head.classification_head.module_list]\n",
    "\n",
    "model.head.classification_head = ssdlite.SSDLiteClassificationHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors_list,  # now a list for each feature map\n",
    "    num_classes=2,\n",
    "    norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "\n",
    "model.head.regression_head = ssdlite.SSDLiteRegressionHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors_list,\n",
    "    norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "\n",
    "\n",
    "# Modify for 1080p Input\n",
    "model.size = (1080, 1920)\n",
    "\n",
    "# Freeze all layers by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the first few layers of the backbone\n",
    "# for layer in list(model.backbone.features)[:1]:  # Modify the number as needed\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last layers of the detection head (classification + box regression)\n",
    "for param in model.head.classification_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.head.regression_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # Print which layers are trainable\n",
    "# trainable_layers = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "# print(\"Trainable layers:\", trainable_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Live plotting function\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(eval_losses, label=\"Eval Loss\", marker=\"o\", linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(0.1)  # Pause to update the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Training Loop with tqdm & Live Graph\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        # tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU if available\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "            # Compute loss\n",
    "            # loss = criterion(outputs, targets)\n",
    "            loss = sum(loss for loss in outputs.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate Model After Each Epoch\n",
    "        avg_eval_loss = evaluate_model(model, test_loader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function with tqdm Progress Bar\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # When targets are provided, the model returns a dictionary of losses.\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "for _ in range(1):\n",
    "    # Train Model with Evaluation Each Epoch\n",
    "    train_model(model, train_loader, test_loader, num_epochs=5)\n",
    "    plot_losses()\n",
    "    torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ONNX with 1080p Input\n",
    "dummy_input = torch.randn(1, 3, 1080, 1920).to(device)  # Adjusted for 1080p\n",
    "torch.onnx.export(model, dummy_input, [\"ssd_1080p.onnx\"], dynamo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\PROJECTS\\MILO\\MILO\\FaceRecognition\\model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
